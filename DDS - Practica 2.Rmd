---
title: "DDS - Práctica 2"
output:
  html_document: default
  pdf_document: default
date: "2024-01-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Data Driven Security - Actividad Evaluable 2

## Víctor López García & Lucas Carrillo Mas

### 1. Datos Elegantes + Análisis de Datos con Web Scrapping

Se listarán a continuación los enunciados de las preguntas requeridas junto con su respuesta.

#### Pregunta 1 - Queremos programar un programa de tipo web scrapping con el que podamos obtener una página web, mediante su URL, y poder analizar su contenido HTML con tal de extraer datos e información específica. Nuestro programa ha de ser capaz de cumplir con los pasos especificados en el pdf de la práctica. Se detalla cada uno con su código a continuación:

##### 1. Descargar la página web de la URL indicada, y almacenarlo en un formato de R apto para ser tratado.

```{r carga paquete}
library(httr)
library(xml2)
library(XML)
library(rvest)
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(dplyr)
```

```{r descarga web}
descarga_web <- GET(url="https://www.mediawiki.org/wiki/MediaWiki")

contenido_web <- content(descarga_web, encoding="UTF-8")

# Lo almaceno en un fichero apto para ser tratado con R --> parseo a XML

contenido_xml <- htmlParse(contenido_web, asText=TRUE)
```

Otra forma más sencilla y con la que continuaremos a lo largo de la práctica, es utilizando la librería "rvest".

```{r descarga web fácil}
contenido_xml <- read_html("https://www.mediawiki.org/wiki/MediaWiki") # Contenido en xml

```

##### 2. Analizar el contenido de la web, buscando el título de la página (que en HTML se etiqueta como “title”).

```{r titulo}
titulo <- html_elements(contenido_xml, "title") %>% html_text()
```

##### 3. Analizar el contenido de la web, buscando todos los enlaces (que en HTML se etiquetan como “a”), buscando el texto del enlace, así como la URL.

```{r enlaces}
links <- html_elements(contenido_xml, "a") %>% html_attr("href")
texto_links <- html_elements(contenido_xml, "a") %>% html_text2()
```

Vemos que existen entradas vacías en "texto_links", que podemos verificar que corresponden a botones en la web (que no tienen texto). Usamos la función "html_text2()" porque elimina los whitespaces y tabulaciones.

##### 4. Generar una tabla con cada enlace encontrado, indicando el texto que acompaña el enlace, y el número de veces que aparece un enlace con ese mismo objetivo.

Vamos a generar una columna que combine la de links y la del texto y contar el número de veces que aparece el mismo link con el mismo texto en la web.

```{r recuento}
df_links <- data.frame(links, texto_links)
#head(df_links)

# Añadir columna que hace una tupla de la columna 1 y la 2 porque quiero contar cuántas veces se repite el mismo link con el mismo texto

df_links$combinado <- paste(df_links$links, df_links$texto_links, sep = " - ")

df_links <- df_links %>%
  mutate(
    links = as.factor(links),
    texto_links = as.factor(texto_links)
  )

tabla_links <- table(df_links$combinado)
#head(tabla_links)
view(df_links)
view(tabla_links)
```

##### 5. Para cada enlace, seguirlo e indicar si está activo (podemos usar el código de status HTTP al hacer una petición a esa URL).

```{r status}

links_clean <- links

# clean special links (start with "#")
links_clean[grepl("^#", links_clean)] <- paste("https://www.mediawiki.org/wiki/MediaWiki", links_clean[grepl("^#", links_clean)], sep="")

# clean special links (start with "//")
links_clean[grepl("^//", links_clean)] <- paste("https:", links_clean[grepl("^//", links_clean)], sep="")

# clean the rest (don't start with "http")
links_clean[!grepl("^http", links_clean)] <- paste("https://www.mediawiki.org", links_clean[!grepl("^http", links_clean)], sep="")


get_status_code <- function(x) {
  Sys.sleep(0.1)
  return(HEAD(x)$status_code)
}

# takes about 1.5 mins to run
print(Sys.time())
status <- sapply(links_clean, get_status_code)
print(Sys.time())


df_clean <- data.frame(links_clean, texto_links, status)

# get the unique links with their frequency (only takes into account the unique "links_clean", not the combination of "links_clean" and "texto_links")
unique_links <- data.frame(table(links_clean))

# join the previous 2 dataframes to assign the correct frequency to each clean link
df_clean <- df_clean %>% left_join(unique_links, by=join_by(links_clean))
# change the column name from "Freq" to "visto"
colnames(df_clean)[4] <- "visto"

# df_clean tiene todas las 172 entradas originales, pero el ejercicio pide los unicos por combinación de links y texto_links
df_clean_unique <- unique(df_clean[,])

```

Notar que aunque cuando haces el "view" sale 172 al final, en realidad tiene 166 filas.

#### Pregunta 2 - Elaborad, usando las librerías de gráficos base y qplot (ggplot2), una infografía sobre los datos obtenidos. Tal infografía será una reunión de gráficos donde se muestren los siguientes detalles:

##### 1. Un histograma con la frecuencia de aparición de los enlaces, pero separado por URLs absolutas (con “http…”) y URLs relativas.

En primer lugar, construimos el gráfico de barras que se nos pide en el enunciado, pero vemos que nos aporta poca información a nivel de análisis de nuestra base de datos, ya que la mayor parte de los enlaces aparecen una única vez. Vamos entonces a aislar todas estas observaciones que son únicas y presentarlas en la tabla "df_links_unicos". Se representa entonces el gráfico de barras de las urls que aparecen en más de una observación.

```{r histograma}
# Crear una nueva columna para indicar si el enlace es absoluto o relativo
df_links$tipo_enlace <- ifelse(grepl("^http", df_links$links) | grepl("^//", df_links$links), "Absoluto", "Relativo")

# Tengo en cuenta que hay urls absolutas que empiezan por http o por // (que serían absolutas también aunque no lo dice el enunciado) 

frec_links <- table(df_links$links)
df_frec_links <- as.data.frame(frec_links)

df_frec_links_sin1 <- subset(df_frec_links, Freq != 1)
df_links_frq1 <- subset(df_frec_links, Freq == 1)
View(df_links_frq1)

# Asigno colores condicionales
colores <- ifelse(df_links$tipo_enlace == "Absoluto", "blue", "red")

# Crear un gráfico de barras con colores condicionales
grafico_barras <- barplot(df_frec_links_sin1$Freq,
                   col = colores,
                   xlab = "Enlaces",
                   main = "Frecuencia de enlaces")

# Añadir leyenda manualmente
legend("topright", legend = c("Absoluto", "Relativo"),
       fill = c("blue", "red"))

# Rotar etiquetas del eje x para leer mejor
par(mar = c(7, 4, 4, 2) + 0.1)
#axis(1, at = 1:length(df_links$links), labels = df_links$links, las = 2, cex.axis = 0.6)

# Definir etiquetas personalizadas (A, B, C, ...)
etiquetas_letras <- LETTERS[1:length(df_frec_links_sin1$Freq)]

# Añadir etiquetas al eje 
axis(1, at = grafico_barras, labels = etiquetas_letras, las = 1, cex.axis = 1, padj = -0.5)


```
### 

Notar que se representan los links con las letras A hasta J para dar un aspecto más limpio al gráfico, la leyenda para saber qué url corresponde a cada letra en el gráfico se deja a continuación:

```{r leyenda urls grafico}

df_frec_links_sin1$EtiquetaGrafico <- LETTERS[1:length(df_frec_links_sin1$Freq)]
view(df_frec_links_sin1)
```

##### 2. Un gráfico de barras indicando la suma de enlaces que apuntan a otros dominios o servicios (distinto a <https://www.mediawiki.org> en el caso de ejemplo) vs. la suma de los otros enlaces.

Gracias al trabajo de homogeneización hecho en el punto 5 de la anterior pregunta, bastaría con representar directamente en un gráfico de barras el número de dominios que empiezan por "<https://www.mediawiki.org>" y los que no en el data frame "df_clean".

```{r barras}
df_clean$mediawiki_links <- grepl("^https://www.mediawiki.org", df_clean$links_clean)

# Asigna nombres personalizados
df_clean$tipo_enlace <- ifelse(df_clean$mediawiki_links, "Internos", "Externos")

# Crea el gráfico de barras con etiquetas debajo
ggplot(df_clean, aes(x = factor(tipo_enlace), fill = factor(mediawiki_links))) +
  geom_bar() +
  geom_text(stat='count', aes(label=..count..), vjust=-0.5) +
  scale_x_discrete(labels = c("Externos", "Internos")) +
  labs(title = "Número de enlaces Internos vs Externos", x = "Tipo de Enlace", y = "Número de Enlaces") +
  theme(legend.position="none",plot.title = element_text(hjust = 0.5))
```

### 

Observamos que hay muchas más enlaces que llevan a sitios internos que a externos.

##### 3. Un gráfico de tarta (pie chart) indicando los porcentajes de Status de nuestro análisis.

```{r pie chart}
# Calculo porcentajes de aparición de cada status en el data frame limpio
porcentajes <- prop.table(table(df_clean$status)) * 100

# Crear el pie chart
ggplot(data = NULL, aes(x = "", y = porcentajes, fill = factor(names(porcentajes)))) +
  geom_bar(stat = "identity", width = 1) +
  geom_text(aes(label = sprintf("%.1f%%", porcentajes)), position = position_stack(vjust = 0.5), color = "black", size = 5) +  # Agregar etiquetas de texto
  coord_polar("y") +
  theme_void() +
  labs(fill = NULL) +
  #scale_fill_manual(values = c("blue", "red", "green")) +  # Puedo ajustar los colores aquí pero me gustan por defecto
  ggtitle("Porcentaje de Aparición de Status") +
  theme(legend.position="right",plot.title = element_text(hjust = 0.5))

```

### 

Observamos que la gran mayoría de los enlaces tienen un código de respuesta de estado satifactorio (200), aunque hay enlaces cuya respuesta es un 404 (sitio no encontrado).

### 2. Análisis de logs de servidor usando R (parte II)

Nuestro programa ha de ser capaz de obtener las respuestas de forma dinámica a las siguientes preguntas utilizando instrucciones de código en R:

##### 1. Descomprimir el fichero comprimido que contiene los registros del servidor, y a partir de los datos extraídos, cargar en data frame los registros con las peticiones servidas.

Es importante notar que se ha modificado el fichero .csv (usando "modify_csv_file.py") pq algunas filas no tenian el formato adecuado (no tienen ningún valor para los bytes, y eso hace que el campo "status" aparaezca como NA). Ahora esas filas tienen "-" al final (igual que el resto de filas, indicando que la respuesta del servidor no tiene bytes)

```{r Obtención y carga de datos}

library(readr)
library(stringr)
library(tidyr)
library(lubridate)

# carga datos
Logs_http <- read_delim("epa-http.csv", delim = " ", show_col_types = FALSE)

# limpieza datos
colnames(Logs_http) <- c("source", "timestamp", "petition", "status", "bytes")
Logs_http <- separate(Logs_http, petition, into = c("type", "url", "protocol"), sep = " ")

Logs_http <- Logs_http %>%
  mutate(
    timestamp = as.POSIXct(timestamp, format="[%d:%H:%M:%S]"),
    type = as.factor(type),
    protocol = as.factor(protocol),
    status = as.integer(status),
    bytes = as.integer(bytes)
  )

# algunos valores de la columna bytes son "-", que indica que el servidor no devuelve ningún dato. Al pasarlo a entero, se convierten en NA, por lo que tenemos que cambiarlos a 0.
Logs_http[is.na(Logs_http$bytes), "bytes"] <- 0


```

##### 3. Aprovechando que los datos a analizar son los mismos de la primera práctica, para esta entrega es imprescindible que los datos estén en formato de “datos elegantes”.

Después de la limpieza hecha, hemos conseguido tener "datos elegantes" que nos serán muy útiles para sacar conclusiones y representar los datos de forma más cómoda.

##### 2. Incluid en el documento un apartado con la descripción de los datos analizados: fuente, tipología, descripción de la información contenida (los diferentes campos) y sus valores.

El fichero .csv importado es una recopilación de logs en un servidor a lo largo de 24h. Tiene 5 columnas:

##### - source: la ip o nombre del que hace la petición, tipo: caracter

##### - timestamp: el dia y hora cuando se hizo la petición, tipo: POSIXct

##### - type: que tipo de petición http es (GET, POST, HEAD, etc), tipo: factor

##### - URL: la URL a la que se hace la petición, tipo: caracter

##### - protocolo: que protocolo se usa en la petición (HTTP/1.0, HTTP/0.2, etc), tipo: factor

##### - status: el status code que devuelve el servidor a la petición (200, 302, 404, etc), tipo: integer

##### - bytes: el numero de bytes que contiene la respuesta del servidor, tipo: integer

##### 4. Identificar el número único de usuarios que han interactuado directamente con el servidor de forma segregada según si los usuarios han tenido algún tipo de error en las distintas peticiones ofrecidas por el servidor.

```{r Exploración de datos}

# use a dataframe for unique user-status pair
unique_Logs_http <- unique(Logs_http[, c("source", "status")])
# take into account that users may be repeated if they had different status codes in different requests


# list of all the users that do have an error somewhere (error status codes are 400s and 500s)
name_filter <- unique_Logs_http[unique_Logs_http$status >= 400, "source"]

# remove the users that have an error, leaving only the users that have no errors
users_no_errors <- filter(unique_Logs_http, !(unique_Logs_http$source %in% name_filter$source))

# remove duplicate users, since we don't care what (non-error) status code they got
users_no_errors <- unique(users_no_errors$source)


# filters users that have at least 1 error
users_errors <- filter(unique_Logs_http, unique_Logs_http$source %in% name_filter$source)

# clean such that only errors are shown
users_errors <- users_errors[users_errors$status >= 400,]

# Notar que "users_errors" tiene algún usuario ("source") duplicado ya que han tenido más de un error diferente en las peticiones que han hecho

```

Los distintos tipos de errores que hay y la cantidad de usuarios para cada uno son:

##### 400 (Bad Request), 1 usuario

##### 403 (Forbidden), 5 usuarios

##### 404 (Not Found), 152 usuarios

##### 500 (Internal Server Error), 29 usuarios

##### 501 (Not Implemented), 11 usuarios

##### Sin Error, 2141 usuarios únicos

##### 5. Analizar los distintos tipos de peticiones HTTP (GET, POST, PUT, DELETE) gestionadas por el servidor, identificando la frecuencia de cada una de estas. Repetir el análisis, esta vez filtrando previamente aquellas peticiones correspondientes a recursos ofrecidos de tipo imagen.

```{r Análisis de Datos}

# get the frequency of "type"
type_frequency <- table(Logs_http$type)

# do the same but filter for urls indicating images are being accessed
# only image formats i found were .jpg and .GIF
images_Logs_http <- filter(Logs_http, grepl(".jpg", Logs_http$url) | grepl(".gif", Logs_http$url))
images_type_frequency <- table(images_Logs_http$type)

```

##### 6. Generar al menos 2 gráficos distintos que permitan visualizar alguna característica relevante de los datos analizados.

```{r representacion}
df_type_frequency <- data.frame(type = names(type_frequency), frequency = as.numeric(type_frequency))

# Crear el gráfico de barras
plot_tot <- ggplot(df_type_frequency, aes(x = type, y = frequency)) +
  geom_bar(stat = "identity", fill = "blue", alpha = 0.7) +
  labs(title = "",
       x = "Tipo de Petición",
       y = "Frecuencia") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))  # Centrar el título
```

```{r representacion images}
df_images_type_frequency <- data.frame(type = names(images_type_frequency), frequency = as.numeric(images_type_frequency))

# Crear el gráfico de barras
plot_img <- ggplot(df_images_type_frequency, aes(x = type, y = frequency)) +
  geom_bar(stat = "identity", fill = "violet", alpha = 0.7) +
  labs(title = "",
       x = "Tipo de Petición",
       y = "Frecuencia") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))  # Centrar el título
```

```{r representacion conjunta}

# Crear el gráfico conjunto
ggarrange(plot_tot,plot_img, labels = c("Total","Solo imágenes"), ncol=2, nrow=1)

```

### 

De estos gráficos observamos que la gran mayoría de las peticiones al servidor son de tipo GET, seguidas de las de tipo POST y finalmente, un número residual de tipo HEAD. Hemos representado la distribución por tipo de petición para todas las observaciones (a la izquierda) y sólamente para las peticiones correspondientes a recursos ofrecidos de tipo imagen (a la derecha) para ver si existía algún tipo de diferencia en la dinámica. La conclusión es que la distribución es muy similar, aunque comparativamente, en las peticiones de imagen, hay menos observaciones de tipo POST.

También podría ser interesante representar los distintos tipos de status de la petición que tenemos en nuestras observaciones y la cantidad de usuarios para cada uno. Para esta representación, podríamos hacer un gráfico circular con porcentajes, pero dada la difenrencia de las peticiones con sin error con respecto al resto, sería poco visual verlo en un pie chart.

Vamos entonces a hacer un gráfico de barras únicamente representando la cantidad de usuarios que hay para cada tipo concreto de error en la muestra.

```{r representacion status}
df_frec_error <- as.data.frame(table(users_errors$status))

# Crear el gráfico de barras
ggplot(df_frec_error, aes(x = Var1, y = Freq)) +
 geom_bar(stat = "identity", fill = "blue", alpha = 0.7) +
  geom_text(aes(label = Freq), vjust = -0.5, color = "black", size = 4) +
  labs(title = "Usuarios por tipo de error",
       x = "Tipo de Error",
       y = "Frecuencia") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))  # Centrar el título
```

### 

Concluimos que el error representativamente más común es el "404 - No encontrado"

##### 7. Generar un gráfico que permita visualizar el número de peticiones servidas a lo largo del tiempo.

```{r representacion num peticiones vs tiempo}

Losg_http_10min <- Logs_http

#Losg_http_10min$timestamp <- as.POSIXct(Losg_http_10min$timestamp, format = "%Y-%m-%d %H:%M:%S")

# Crear intervalos de 10 minutos

Losg_http_10min <- Losg_http_10min %>%
  mutate(intervalo_tiempo = as.POSIXct(cut(timestamp, breaks = "10 min"))) %>%
  group_by(intervalo_tiempo) %>%
  summarise(num_peticiones = n())

# Ahora, puedes crear el gráfico de barras
ggplot(Losg_http_10min, aes(x = intervalo_tiempo, y = num_peticiones)) +
  geom_bar(stat = "identity") +
  labs(title = "Número de Peticiones en Intervalos de 10 Minutos",
       x = "Intervalo de Tiempo",
       y = "Número de Peticiones") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + # Rotar etiquetas del eje x para mejor legibilidad
  theme(plot.title = element_text(hjust = 0.5)) +  # Centrar el título
  scale_x_datetime(date_labels = "%m-%d %H:%M", date_breaks = "1 hour")  # Mostrar una etiqueta cada hora
```

### 

Vemos de manera muy visual que la mayor parte de peticiones de concentran en las horas centrales del día, entre las 11 de la mañana y las 5 de la tarde.

### 3. Clústering de datos

##### 8. Utilizando un algoritmo de aprendizaje no supervisado, realizad un análisis de clústering con k-means para los datos del servidor.

```{r carga paquetes}
library(mltools)
library(data.table)
```

En primer lugar, vamos a fijar una semilla para que la asignación aleatoria de los centroides en la primera iteración del algoritmo sea simpre la misma.Utilizamos la función one_hot() para convertir las columnas que teníamos como factor en numéricas representando el valor de la variable factor de forma que pueda usarse en algoritmos como k-means que trabaja únicamente con valores numéricos.También descartamos los valores NA y nos quedamos con las 8 columnas que nos interesan para el clustering: "type_GET", "type_HEAD", "type_POST", "protocol_HTTP/0.2", "protocol_HTTP/1.0", "status", "bytes", "url_count"; siendo esta última la cuenta del número de caracteres de la columna con la url de la petición.

Finalmente, utilizamos la función kmeans() con dos valores distintos de k (número de clusters). Imprimiendo el output de esa función podemos ver cómo nos ha agrupado los clusters, sus centros, tamaños, etc.

```{r clustering}
set.seed(124)
Logs_http_one_hot <- one_hot(as.data.table(Logs_http), sparsifyNAs = TRUE)

Logs_http_one_hot$url_count <- nchar(Logs_http_one_hot$url)
clean_Logs_http_one_hot <- Logs_http_one_hot[,c("type_GET", "type_HEAD", "type_POST", "protocol_HTTP/0.2", "protocol_HTTP/1.0", "status", "bytes", "url_count")]

k_means_3 <- kmeans(clean_Logs_http_one_hot, 3)
k_means_5 <- kmeans(clean_Logs_http_one_hot, 5)

#k_means_3

```

##### 9. Representad visualmente en gráficos de tipo scatter plot el resultado de vuestros clústering y interpretad el resultado obtenido

Haremos representaciones gráficas de los clústers del anterior apartado.

En primer lugar, haciendo un análisis preliminar del Data Frame vemos que las 5 primeras variables no van a tener sentido ninguno para la representación de su clusterización, pues se trata únicamente de valores 0 o 1. Además, la columna status también tendrá poco sentido por contener a lo sumo 5 tipos diferentes de categorías (aunque las tengamos en tipo numérico).

De todas maneras, vamos a representar todos los posibles casos dos a dos para ver cuál nos da más información. Utilizamos k=3 de ahora en adelante:

```{r clust tot}
plot(clean_Logs_http_one_hot, col=k_means_3$cluster)
```

### 

Parece que la proyección más interesante y la que muestra la variable predominante en la clusterización es la de "bytes" vs "url_count", por lo que nos vamos a centrar en ella.

```{r cluster representacion}
df_centros <- data.frame(k_means_3[2]) # Buscamos los centros para representarlos encima del plot si queremos

k_means_3$cluster <- as.factor(k_means_3$cluster)
cluster <- k_means_3$cluster

ggplot(clean_Logs_http_one_hot, aes(clean_Logs_http_one_hot$url_count, clean_Logs_http_one_hot$bytes, col=cluster)) + geom_point() + labs(x = "Numero de caracteres en la URL", y = "Bytes en la respuesta")

```

### 

Si pintamos las funciones de densidad de ambas variables, vemos que esta agrupación puede tener sentido, ya que la gran mayoría de las observaciones se concentran en números de bytes pequeños, por lo que estarán más juntas los puntos cerca del eje x de la representación, por lo que la distancia entre ellas será tan pequeña que caerán en el mismo cluster.

```{r repr cluster density}
plot(density(clean_Logs_http_one_hot$url_count))
plot(density(clean_Logs_http_one_hot$bytes))
```

### 

Adicionalmente, se nos ha ocurrido que quizás esta inlfuencia masiva de la variable "bytes" en la clusterización venga dada por los valores tan altos que tienen sus observaciones comparadas con el resto de variables. Vamos entonces a normalizar los datos de las variables "bytes" y "url_count".

```{r repr cluster norm}
set.seed(122)

df_norm <- clean_Logs_http_one_hot
df_norm$bytes_norm <- (clean_Logs_http_one_hot$bytes)/max(clean_Logs_http_one_hot$bytes)*100
df_norm$url_count_norm <- (clean_Logs_http_one_hot$url_count)/max(clean_Logs_http_one_hot$url_count)*100
df_norm <- df_norm[,!c("url_count", "bytes")]

k_means_norm_3 <- kmeans(df_norm, 3)
k_means_norm_3$cluster <- as.factor(k_means_norm_3$cluster)
df_centros <- data.frame(k_means_norm_3[2])
cluster <- k_means_norm_3$cluster

ggplot(df_norm, aes(df_norm$url_count_norm, df_norm$bytes_norm, col=cluster)) + geom_point() +
   labs(x = "Numero de caracteres en la URL", y = "Bytes en la respuesta")

# Para ver qué pasa muy abajo en el eje y (columna bytes)
#plot(data.frame(df_norm$url_count_norm, df_norm$bytes_norm), col=k_means_norm_3$cluster, ylim=c(0,0.005))
#points(k_means_norm_3$centers[,1], k_means_norm_3$centers[,2], col="red")
```

### 

Vemos ahora que tenemos un cluster cerca del punto (0,0), lo que concuerda con las funciones de densidad pintadas anteriormente que nos decían que casi todas las observaciones se situaban en valores pequeños de bytes y de caracteres de la url.
